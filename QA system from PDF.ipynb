{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tutorial-use-pdf-converter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx0Sg_fRM5Kh"
      },
      "source": [
        "# Notebook [2]: Using the PDF converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5PrFhdQMeBF"
      },
      "source": [
        "\n",
        "\n",
        "This notebook shows how to use the PDF converter to create an input dataframe for the cdQA pipeline from a directory of PDF files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f58-FXmbMfjz"
      },
      "source": [
        "***Note:*** *To run this notebook you will need to have access to GPU. If you are using colab, you will need to install `cdQA` by executing `!pip install cdqa` in a cell.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kfF5AD5z8W2"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from ast import literal_eval"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ErW3w_0tAL",
        "outputId": "09a4ead7-79b1-42bb-d3b8-faa93acc0ddc"
      },
      "source": [
        "!pip install tika"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (50.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.11.8)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32884 sha256=8c12b4ec5f7b7816dd43478e4be217af0a73b96224aac50e1ec61d7cac97f474\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVb4_hK5z8Kl"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from tika import parser\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import markdown\n",
        "from pathlib import Path\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "\n",
        "def df2squad(df, squad_version=\"v1.1\", output_dir=None, filename=None):\n",
        "    \"\"\"\n",
        "     Converts a pandas dataframe with columns ['title', 'paragraphs'] to a json file with SQuAD format.\n",
        "     Parameters\n",
        "    ----------\n",
        "     df : pandas.DataFrame\n",
        "         a pandas dataframe with columns ['title', 'paragraphs']\n",
        "     squad_version : str, optional\n",
        "         the SQuAD dataset version format (the default is 'v2.0')\n",
        "     output_dir : str, optional\n",
        "         Enable export of output (the default is None)\n",
        "     filename : str, optional\n",
        "         [description]\n",
        "    Returns\n",
        "    -------\n",
        "    json_data: dict\n",
        "        A json object with SQuAD format\n",
        "     Examples\n",
        "     --------\n",
        "     >>> from ast import literal_eval\n",
        "     >>> import pandas as pd\n",
        "     >>> from cdqa.utils.converters import df2squad\n",
        "     >>> from cdqa.utils.filters import filter_paragraphs\n",
        "     >>> df = pd.read_csv('../data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv', converters={'paragraphs': literal_eval})\n",
        "     >>> df['paragraphs'] = df['paragraphs'].apply(filter_paragraphs)\n",
        "     >>> json_data = df2squad(df=df, squad_version='v1.1', output_dir='../data', filename='bnpp_newsroom-v1.1')\n",
        "    \"\"\"\n",
        "\n",
        "    json_data = {}\n",
        "    json_data[\"version\"] = squad_version\n",
        "    json_data[\"data\"] = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows()):\n",
        "        temp = {\"title\": row[\"title\"], \"paragraphs\": []}\n",
        "        for paragraph in row[\"paragraphs\"]:\n",
        "            temp[\"paragraphs\"].append({\"context\": paragraph, \"qas\": []})\n",
        "        json_data[\"data\"].append(temp)\n",
        "\n",
        "    if output_dir:\n",
        "        with open(os.path.join(output_dir, \"{}.json\".format(filename)), \"w\") as outfile:\n",
        "            json.dump(json_data, outfile)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def generate_squad_examples(question, best_idx_scores, metadata, retrieve_by_doc):\n",
        "    \"\"\"\n",
        "    Creates a SQuAD examples json object for a given question using outputs of retriever and document database.\n",
        "    Parameters\n",
        "    ----------\n",
        "    question : [type]\n",
        "        [description]\n",
        "    best_idx_scores : [type]\n",
        "        [description]\n",
        "    metadata : [type]\n",
        "        [description]\n",
        "    Returns\n",
        "    -------\n",
        "    squad_examples: list\n",
        "        [description]\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from cdqa.utils.converters import generate_squad_examples\n",
        "    >>> squad_examples = generate_squad_examples(question='Since when does the the Excellence Program of BNP Paribas exist?',\n",
        "                                         best_idx_scores=[(788, 1.2), (408, 0.4), (2419, 0.2)],\n",
        "                                         metadata=df)\n",
        "    \"\"\"\n",
        "\n",
        "    squad_examples = []\n",
        "\n",
        "    metadata_sliced = metadata.loc[best_idx_scores.keys()]\n",
        "\n",
        "    for idx, row in metadata_sliced.iterrows():\n",
        "        temp = {\"title\": row[\"title\"], \"paragraphs\": []}\n",
        "\n",
        "        if retrieve_by_doc:\n",
        "            for paragraph in row[\"paragraphs\"]:\n",
        "                temp[\"paragraphs\"].append(\n",
        "                    {\n",
        "                        \"context\": paragraph,\n",
        "                        \"qas\": [\n",
        "                            {\n",
        "                                \"answers\": [],\n",
        "                                \"question\": question,\n",
        "                                \"id\": str(uuid.uuid4()),\n",
        "                                \"retriever_score\": best_idx_scores[idx],\n",
        "                            }\n",
        "                        ],\n",
        "                    }\n",
        "                )\n",
        "        else:\n",
        "            temp[\"paragraphs\"] = [\n",
        "                {\n",
        "                    \"context\": row[\"content\"],\n",
        "                    \"qas\": [\n",
        "                        {\n",
        "                            \"answers\": [],\n",
        "                            \"question\": question,\n",
        "                            \"id\": str(uuid.uuid4()),\n",
        "                            \"retriever_score\": best_idx_scores[idx],\n",
        "                        }\n",
        "                    ],\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        squad_examples.append(temp)\n",
        "\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def pdf_converter(directory_path, min_length=200, include_line_breaks=False):\n",
        "    \"\"\"\n",
        "    Function to convert PDFs to Dataframe with columns as title & paragraphs.\n",
        "    Parameters\n",
        "    ----------\n",
        "    min_length : integer\n",
        "        Minimum character length to be considered as a single paragraph\n",
        "    include_line_breaks: bool\n",
        "        To concatenate paragraphs less than min_length to a single paragraph\n",
        "    Returns\n",
        "    -------------\n",
        "    df : Dataframe\n",
        "    Description\n",
        "    -----------------\n",
        "    If include_line_breaks is set to True, paragraphs with character length\n",
        "    less than min_length (minimum character length of a paragraph) will be\n",
        "    considered as a line. Lines before or after each paragraph(length greater\n",
        "    than or equal to min_length) will be concatenated to a single paragraph to\n",
        "    form the list of paragraphs in Dataframe.\n",
        "    Else paragraphs are appended directly to form the list.\n",
        "    \"\"\"\n",
        "    list_file = os.listdir(directory_path)\n",
        "    list_pdf = []\n",
        "    for file in list_file:\n",
        "        if file.endswith(\"pdf\"):\n",
        "            list_pdf.append(file)\n",
        "    df = pd.DataFrame(columns=[\"title\", \"paragraphs\"])\n",
        "    for i, pdf in enumerate(list_pdf):\n",
        "        try:\n",
        "            df.loc[i] = [pdf.replace(\".pdf\",''), None]\n",
        "            raw = parser.from_file(os.path.join(directory_path, pdf))\n",
        "            s = raw[\"content\"].strip()\n",
        "            paragraphs = re.split(\"\\n\\n(?=\\u2028|[A-Z-0-9])\", s)\n",
        "            list_par = []\n",
        "            temp_para = \"\"  # variable that stores paragraphs with length<min_length\n",
        "            # (considered as a line)\n",
        "            for p in paragraphs:\n",
        "                if not p.isspace():  # checking if paragraph is not only spaces\n",
        "                    if include_line_breaks:  # if True, check length of paragraph\n",
        "                        if len(p) >= min_length:\n",
        "                            if temp_para:\n",
        "                                # if True, append temp_para which holds concatenated\n",
        "                                # lines to form a paragraph before current paragraph p\n",
        "                                list_par.append(temp_para.strip())\n",
        "                                temp_para = (\n",
        "                                    \"\"\n",
        "                                )  # reset temp_para for new lines to be concatenated\n",
        "                                list_par.append(\n",
        "                                    p.replace(\"\\n\", \"\")\n",
        "                                )  # append current paragraph with length>min_length\n",
        "                            else:\n",
        "                                list_par.append(p.replace(\"\\n\", \"\"))\n",
        "                        else:\n",
        "                            # paragraph p (line) is concatenated to temp_para\n",
        "                            line = p.replace(\"\\n\", \" \").strip()\n",
        "                            temp_para = temp_para + f\" {line}\"\n",
        "                    else:\n",
        "                        # appending paragraph p as is to list_par\n",
        "                        list_par.append(p.replace(\"\\n\", \"\"))\n",
        "                else:\n",
        "                    if temp_para:\n",
        "                        list_par.append(temp_para.strip())\n",
        "\n",
        "            df.loc[i, \"paragraphs\"] = list_par\n",
        "        except:\n",
        "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "            print(\"Unable to process file {}\".format(pdf))\n",
        "    return df\n",
        "\n",
        "\n",
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.fed = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return \"\".join(self.fed)\n",
        "\n",
        "\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()\n",
        "\n",
        "\n",
        "def md_converter(directory_path):\n",
        "    \"\"\"Get all md, convert them to html and create the pandas dataframe with columns ['title', 'paragraphs']\"\"\"\n",
        "    dict_doc = {\"title\": [], \"paragraphs\": []}\n",
        "    for md_file in Path(directory_path).glob(\"**/*.md\"):\n",
        "        md_file = str(md_file)\n",
        "        filename = md_file.split(\"/\")[-1]\n",
        "        try:\n",
        "            with open(md_file, \"r\") as f:\n",
        "                dict_doc[\"title\"].append(filename)\n",
        "                md_text = f.read()\n",
        "                html_text = markdown.markdown(md_text)\n",
        "                html_text_list = list(html_text.split(\"<p>\"))\n",
        "                for i in range(len(html_text_list)):\n",
        "                    html_text_list[i] = (\n",
        "                        strip_tags(html_text_list[i])\n",
        "                        .replace(\"\\n\", \" \")\n",
        "                        .lstrip()\n",
        "                        .rstrip()\n",
        "                    )\n",
        "                clean_text_list = list(filter(None, html_text_list))\n",
        "                dict_doc[\"paragraphs\"].append(clean_text_list)\n",
        "        except:\n",
        "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "            print(\"Unable to process file {}\".format(filename))\n",
        "    df = pd.DataFrame.from_dict(dict_doc)\n",
        "    return df\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKUB_YRu1UcZ"
      },
      "source": [
        "#from cdqa.utils.filters import filter_paragraphs\n",
        "  \n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def filter_paragraphs(\n",
        "    articles,\n",
        "    drop_empty=True,\n",
        "    read_threshold=1000,\n",
        "    public_data=True,\n",
        "    min_length=50,\n",
        "    max_length=300,\n",
        "):\n",
        "    \"\"\"\n",
        "    Cleans the paragraphs and filters them regarding their length\n",
        "    Parameters\n",
        "    ----------\n",
        "    articles : DataFrame of all the articles \n",
        "    Returns\n",
        "    -------\n",
        "    Cleaned and filtered dataframe\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> from cdqa.utils.filters import filter_paragraphs\n",
        "    >>> df = pd.read_csv('data.csv')\n",
        "    >>> df_cleaned = filter_paragraphs(df)\n",
        "    \"\"\"\n",
        "\n",
        "    # Replace and split\n",
        "    def replace_and_split(paragraphs):\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph.replace(\"'s\", \" \" \"s\").replace(\"\\\\n\", \"\").split(\"'\")\n",
        "        return paragraphs\n",
        "\n",
        "    # Select paragraphs with the required size\n",
        "    def filter_on_size(paragraphs, min_length=min_length, max_length=max_length):\n",
        "        paragraph_filtered = [\n",
        "            paragraph.strip()\n",
        "            for paragraph in paragraphs\n",
        "            if len(paragraph.split()) >= min_length\n",
        "            and len(paragraph.split()) <= max_length\n",
        "        ]\n",
        "        return paragraph_filtered\n",
        "\n",
        "    # Cleaning and filtering\n",
        "    articles[\"paragraphs\"] = articles[\"paragraphs\"].apply(replace_and_split)\n",
        "    articles[\"paragraphs\"] = articles[\"paragraphs\"].apply(filter_on_size)\n",
        "    articles[\"paragraphs\"] = articles[\"paragraphs\"].apply(\n",
        "        lambda x: x if len(x) > 0 else np.nan\n",
        "    )\n",
        "\n",
        "    # Read threshold for private dataset\n",
        "    if not public_data:\n",
        "        articles = articles.loc[articles[\"number_of_read\"] >= read_threshold]\n",
        "\n",
        "    # Drop empty articles\n",
        "    if drop_empty:\n",
        "        articles = articles.dropna(subset=[\"paragraphs\"]).reset_index(drop=True)\n",
        "\n",
        "    return articles"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJ8Et59L34tL",
        "outputId": "36179b99-096d-48f4-ba42-88533d9a56e3"
      },
      "source": [
        "!pip install cdqa\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cdqa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/f5/af831b7ee653aa6bace99e39ec6b2754b1adb10bb60a1296f5e16f1f24ee/cdqa-1.3.9.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hCollecting Flask==1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.4MB/s \n",
            "\u001b[?25hCollecting flask_cors==3.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting joblib==0.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 8.5MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/9a/7eb9952f4b4d73fbd75ad1d5d6112f407e695957444cb695cbb3cdab918a/pandas-0.25.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 13.5MB/s \n",
            "\u001b[?25hCollecting prettytable==0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting transformers==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 54.2MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 23.6MB/s \n",
            "\u001b[?25hCollecting tika==1.19\n",
            "  Downloading https://files.pythonhosted.org/packages/10/75/b566e446ffcf292f10c8d84c15a3d91615fe3d7ca8072a17c949d4e84b66/tika-1.19.tar.gz\n",
            "\u001b[33m  WARNING: Requested tika==1.19 from https://files.pythonhosted.org/packages/10/75/b566e446ffcf292f10c8d84c15a3d91615fe3d7ca8072a17c949d4e84b66/tika-1.19.tar.gz#sha256=a1bac3eb54ea99b672dc1c097f7d10748748bf3a58e2def7a190fefed9c49324 (from cdqa), but installing version 1.24\u001b[0m\n",
            "Collecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 24kB/s \n",
            "\u001b[?25hCollecting markdown==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.2MB/s \n",
            "\u001b[?25hCollecting tqdm==4.32.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->cdqa) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->cdqa) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->cdqa) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->cdqa) (2.11.2)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.6/dist-packages (from flask_cors==3.0.8->cdqa) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.0->cdqa) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.0->cdqa) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.0->cdqa) (2018.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->cdqa) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->cdqa) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.3MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/a1/d8b9be4f3996265cb4e42dcd0ba72d61d68062ed6d8ce7e26b37cc399455/boto3-1.16.24-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 61.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 64.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit_learn==0.21.2->cdqa) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika==1.19->cdqa) (50.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask==1.1.1->cdqa) (1.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1->cdqa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1->cdqa) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1->cdqa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1->cdqa) (2.10)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/16/8afd6474045f41bd51002e65862e2066ea2c5de34d0a942e1c4e86098d9a/botocore-1.19.24-py2.py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 58.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: cdqa, prettytable, tika, wget, sacremoses\n",
            "  Building wheel for cdqa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdqa: filename=cdqa-1.3.9-cp36-none-any.whl size=47640 sha256=bdbd8bc32656e75ded83736fd4f2ffe3339c1b902a5c0a7d6de5a8ea253643f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/9a/68/d3f7651ea29c30d1bebc9e946bf5a8cf922e1c86fb6b8a33d9\n",
            "  Building wheel for prettytable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prettytable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=60f8baf318015a0923105186d7d955e4d70348326099478f6b35dd0835712d04\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=29223 sha256=2fdc3de8bdfb6668f38317def6509a566431bb6b267b2fc652476be7fd739ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/db/8a/3a3f0c0725448eaa92703e3dda71e29dc13a119ff6c1036848\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=90d5c0c96fbfeba0d92920dd2063990cf72e1ef2d9932c48371305fecdc23965\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b43da0c7142b0737dba24ec492b0ada5be526073330fc7dec6317993b9754eed\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built cdqa prettytable tika wget sacremoses\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.32.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.32.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.24 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cdqa 1.3.9 has requirement tika==1.19, but you'll have tika 1.24 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Flask, flask-cors, joblib, pandas, prettytable, tqdm, sacremoses, jmespath, botocore, s3transfer, boto3, sentencepiece, transformers, scikit-learn, tika, torch, markdown, wget, cdqa\n",
            "  Found existing installation: Flask 1.1.2\n",
            "    Uninstalling Flask-1.1.2:\n",
            "      Successfully uninstalled Flask-1.1.2\n",
            "  Found existing installation: joblib 0.17.0\n",
            "    Uninstalling joblib-0.17.0:\n",
            "      Successfully uninstalled joblib-0.17.0\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: tika 1.24\n",
            "    Uninstalling tika-1.24:\n",
            "      Successfully uninstalled tika-1.24\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: Markdown 3.3.3\n",
            "    Uninstalling Markdown-3.3.3:\n",
            "      Successfully uninstalled Markdown-3.3.3\n",
            "Successfully installed Flask-1.1.1 boto3-1.16.24 botocore-1.19.24 cdqa-1.3.9 flask-cors-3.0.8 jmespath-0.10.0 joblib-0.13.2 markdown-3.1.1 pandas-0.25.0 prettytable-0.7.2 s3transfer-0.3.3 sacremoses-0.0.43 scikit-learn-0.21.2 sentencepiece-0.1.94 tika-1.24 torch-1.2.0 tqdm-4.32.2 transformers-2.1.1 wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "markdown",
                  "pandas",
                  "sklearn",
                  "tika",
                  "torch",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYmn95v3fti"
      },
      "source": [
        "import joblib\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "from cdqa.retriever import TfidfRetriever, BM25Retriever\n",
        "from cdqa.utils.converters import generate_squad_examples\n",
        "from cdqa.reader import BertProcessor, BertQA\n",
        "\n",
        "RETRIEVERS = {\"bm25\": BM25Retriever, \"tfidf\": TfidfRetriever}\n",
        "\n",
        "\n",
        "class QAPipeline(BaseEstimator):\n",
        "    \"\"\"\n",
        "    A scikit-learn implementation of the whole cdQA pipeline\n",
        "    Parameters\n",
        "    ----------\n",
        "    reader: str (path to .joblib) or .joblib object of an instance of BertQA (BERT model with sklearn wrapper), optional\n",
        "    retriever: \"bm25\" or \"tfidf\"\n",
        "        The type of retriever\n",
        "    retrieve_by_doc: bool (default: True). If Retriever will rank by documents\n",
        "        or by paragraphs.\n",
        "    kwargs: kwargs for BertQA(), BertProcessor(), TfidfRetriever() and BM25Retriever()\n",
        "        Please check documentation for these classes\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from cdqa.pipeline import QAPipeline\n",
        "    >>> qa_pipeline = QAPipeline(reader='bert_qa_squad_vCPU-sklearn.joblib')\n",
        "    >>> qa_pipeline.fit_retriever(df=df)\n",
        "    >>> prediction = qa_pipeline.predict(query='When BNP Paribas was created?')\n",
        "    >>> from cdqa.pipeline import QAPipeline\n",
        "    >>> qa_pipeline = QAPipeline()\n",
        "    >>> qa_pipeline.fit_reader('train-v1.1.json')\n",
        "    >>> qa_pipeline.fit_retriever(df=df)\n",
        "    >>> prediction = qa_pipeline.predict(X='When BNP Paribas was created?')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reader=None, retriever=\"bm25\", retrieve_by_doc=False, **kwargs):\n",
        "\n",
        "        if retriever not in RETRIEVERS:\n",
        "            raise ValueError(\n",
        "                \"You provided a type of retriever that is not supported. \"\n",
        "                + \"Please provide a retriver in the following list: \"\n",
        "                + str(list(RETRIEVERS.keys()))\n",
        "            )\n",
        "\n",
        "        retriever_class = RETRIEVERS[retriever]\n",
        "\n",
        "        # Separating kwargs\n",
        "        kwargs_bertqa = {\n",
        "            key: value\n",
        "            for key, value in kwargs.items()\n",
        "            if key in BertQA.__init__.__code__.co_varnames\n",
        "        }\n",
        "\n",
        "        kwargs_processor = {\n",
        "            key: value\n",
        "            for key, value in kwargs.items()\n",
        "            if key in BertProcessor.__init__.__code__.co_varnames\n",
        "        }\n",
        "\n",
        "        kwargs_retriever = {\n",
        "            key: value\n",
        "            for key, value in kwargs.items()\n",
        "            if key in retriever_class.__init__.__code__.co_varnames\n",
        "        }\n",
        "\n",
        "        if not reader:\n",
        "            self.reader = BertQA(**kwargs_bertqa)\n",
        "        elif type(reader) == str:\n",
        "            self.reader = joblib.load(reader)\n",
        "        else:\n",
        "            self.reader = reader\n",
        "\n",
        "        self.processor_train = BertProcessor(is_training=True, **kwargs_processor)\n",
        "\n",
        "        self.processor_predict = BertProcessor(is_training=False, **kwargs_processor)\n",
        "\n",
        "        self.retriever = retriever_class(**kwargs_retriever)\n",
        "\n",
        "        self.retrieve_by_doc = retrieve_by_doc\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    def fit_retriever(self, df: pd.DataFrame = None):\n",
        "        \"\"\" Fit the QAPipeline retriever to a list of documents in a dataframe.\n",
        "        Parameters\n",
        "        ----------\n",
        "        df: pandas.Dataframe\n",
        "            Dataframe with the following columns: \"title\", \"paragraphs\"\n",
        "        \"\"\"\n",
        "\n",
        "        if self.retrieve_by_doc:\n",
        "            self.metadata = df\n",
        "            self.metadata[\"content\"] = self.metadata[\"paragraphs\"].apply(\n",
        "                lambda x: \" \".join(x)\n",
        "            )\n",
        "        else:\n",
        "            self.metadata = self._expand_paragraphs(df)\n",
        "\n",
        "        self.retriever.fit(self.metadata)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def fit_reader(self, data=None):\n",
        "        \"\"\" Fit the QAPipeline retriever to a list of documents in a dataframe.\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: dict str-path to json file\n",
        "             Annotated dataset in squad-like for Reader training\n",
        "        \"\"\"\n",
        "\n",
        "        train_examples, train_features = self.processor_train.fit_transform(data)\n",
        "        self.reader.fit(X=(train_examples, train_features))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        query: str = None,\n",
        "        n_predictions: int = None,\n",
        "        retriever_score_weight: float = 0.35,\n",
        "        return_all_preds: bool = False,\n",
        "    ):\n",
        "        \"\"\" Compute prediction of an answer to a question\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: str\n",
        "            Sample (question) to perform a prediction on\n",
        "        n_predictions: int or None (default: None).\n",
        "            Number of returned predictions. If None, only one prediction is return\n",
        "        retriever_score_weight: float (default: 0.35).\n",
        "            The weight of retriever score in the final score used for prediction.\n",
        "            Given retriever score and reader average of start and end logits, the final score used for ranking is:\n",
        "            final_score = retriever_score_weight * retriever_score + (1 - retriever_score_weight) * (reader_avg_logit)\n",
        "        return_all_preds: boolean (default: False)\n",
        "            whether to return a list of all predictions done by the Reader or not\n",
        "        Returns\n",
        "        -------\n",
        "        if return_all_preds is False:\n",
        "        prediction: tuple (answer, title, paragraph, score/logit)\n",
        "        if return_all_preds is True:\n",
        "        List of dictionnaries with all metadada of all answers outputted by the Reader\n",
        "        given the question.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(query, str):\n",
        "            raise TypeError(\n",
        "                \"The input is not a string. Please provide a string as input.\"\n",
        "            )\n",
        "        if not (\n",
        "            isinstance(n_predictions, int) or n_predictions is None or n_predictions < 1\n",
        "        ):\n",
        "            raise TypeError(\"n_predictions should be a positive Integer or None\")\n",
        "        best_idx_scores = self.retriever.predict(query)\n",
        "        squad_examples = generate_squad_examples(\n",
        "            question=query,\n",
        "            best_idx_scores=best_idx_scores,\n",
        "            metadata=self.metadata,\n",
        "            retrieve_by_doc=self.retrieve_by_doc,\n",
        "        )\n",
        "        examples, features = self.processor_predict.fit_transform(X=squad_examples)\n",
        "        prediction = self.reader.predict(\n",
        "            X=(examples, features),\n",
        "            n_predictions=n_predictions,\n",
        "            retriever_score_weight=retriever_score_weight,\n",
        "            return_all_preds=return_all_preds,\n",
        "        )\n",
        "        return prediction\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\" Send reader to CPU if device=='cpu' or to GPU if device=='cuda'\n",
        "        \"\"\"\n",
        "        if device not in (\"cpu\", \"cuda\"):\n",
        "            raise ValueError(\"Attribute device should be 'cpu' or 'cuda'.\")\n",
        "\n",
        "        self.reader.model.to(device)\n",
        "        self.reader.device = torch.device(device)\n",
        "        return self\n",
        "\n",
        "    def cpu(self):\n",
        "        \"\"\" Send reader to CPU\n",
        "        \"\"\"\n",
        "        self.reader.model.cpu()\n",
        "        self.reader.device = torch.device(\"cpu\")\n",
        "        return self\n",
        "\n",
        "    def cuda(self):\n",
        "        \"\"\" Send reader to GPU\n",
        "        \"\"\"\n",
        "        self.reader.model.cuda()\n",
        "        self.reader.device = torch.device(\"cuda\")\n",
        "        return self\n",
        "\n",
        "    def dump_reader(self, filename):\n",
        "        \"\"\" Dump reader model to a .joblib object\n",
        "        \"\"\"\n",
        "        self.cpu()\n",
        "        joblib.dump(self.reader, filename)\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    @staticmethod\n",
        "    def _expand_paragraphs(df):\n",
        "        # Snippet taken from: https://stackoverflow.com/a/48532692/11514226\n",
        "        lst_col = \"paragraphs\"\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                col: np.repeat(df[col].values, df[lst_col].str.len())\n",
        "                for col in df.columns.drop(lst_col)\n",
        "            }\n",
        "        ).assign(**{lst_col: np.concatenate(df[lst_col].values)})[df.columns]\n",
        "        df[\"content\"] = df[\"paragraphs\"]\n",
        "        return df.drop(\"paragraphs\", axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA87Ixvz28Z7"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf3lntW727wT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:41:40.814076Z",
          "start_time": "2019-07-20T13:41:39.440654Z"
        },
        "collapsed": true,
        "id": "7UMrjUJ2EGmu"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "\n",
        "from cdqa.pipeline import QAPipeline\n",
        "from cdqa.utils.download import download_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1fV_dquOrx0"
      },
      "source": [
        "### Download pre-trained reader model and PDF files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:42:54.139892Z",
          "start_time": "2019-07-20T13:41:41.869993Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRcSxZeyiif",
        "outputId": "a90492b0-a5e8-4453-8059-3513fec79295"
      },
      "source": [
        "# Download model\n",
        "download_model(model='bert-squad_1.1', dir='./models')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading trained model...\n",
            "bert_qa.joblib already downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:43:21.153039Z",
          "start_time": "2019-07-20T13:43:20.228398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhg8jFjbERzv",
        "outputId": "5994c11f-9e41-46a9-fb94-5e766bdb06c3"
      },
      "source": [
        "# Download pdf files from BNP Paribas public news\n",
        "def download_pdf():\n",
        "    import os\n",
        "    import wget\n",
        "    directory = './data/pdf/'\n",
        "    models_url = [\n",
        "      'https://sbi.co.in/documents/17826/35696/23062020_SBI+AR+2019-20+%28Time+16_3b11%29.pdf/a358b5ec-1d32-a093-d9ac-13071fda9ff6?t=1592911831224',\n",
        "      'https://sbi.co.in/documents/17826/24027/2007201345-SBI+Sustainability+Report+V37+20_07_2020_Spread_layout.pdf/801cc0de-a47d-c860-f5c3-fc57efb58339?t=1595232977158'\n",
        "      \n",
        "    ]\n",
        "\n",
        "    print('\\nDownloading PDF files...')\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    for url in models_url:\n",
        "        wget.download(url=url, out=directory)\n",
        "\n",
        "download_pdf()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading PDF files...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqPK6BV2O-RO"
      },
      "source": [
        "### Convert the PDF files into a DataFrame for cdQA pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:44:01.821890Z",
          "start_time": "2019-07-20T13:43:22.685954Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "czafu4-aEXXm",
        "outputId": "88220d5a-5180-4475-8d77-ca060179f8be"
      },
      "source": [
        "df = pdf_converter(directory_path='./data/pdf/')\n",
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>paragraphs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23062020_SBI AR 2019-20 (Time 16_3b11)</td>\n",
              "      <td>[ANNUAL REPORT 2019-20, STATE BANK OF INDIA, E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    title                                         paragraphs\n",
              "0  23062020_SBI AR 2019-20 (Time 16_3b11)  [ANNUAL REPORT 2019-20, STATE BANK OF INDIA, E..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLZd4H_vPJuU"
      },
      "source": [
        "### Instantiate the cdQA pipeline from a pre-trained reader model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:44:46.283172Z",
          "start_time": "2019-07-20T13:44:45.317024Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OOqnkNyEaFe",
        "outputId": "29412852-b841-40e1-d00b-ef1b2beae8ce"
      },
      "source": [
        "cdqa_pipeline = QAPipeline(reader='./models/bert_qa.joblib', max_df=1.0)\n",
        "\n",
        "# Fit Retriever to documents\n",
        "cdqa_pipeline.fit_retriever(df=df)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1161595.60B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
              "                         do_lower_case=True, fp16=False,\n",
              "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
              "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
              "                         n_best_size=20, no_cuda=False,\n",
              "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
              "                         output_dir=None, predict_batch_size=8, seed=42,\n",
              "                         server_ip='', server_po..._size=8,\n",
              "                         verbose_logging=False, version_2_with_negative=False,\n",
              "                         warmup_proportion=0.1, warmup_steps=0),\n",
              "           retrieve_by_doc=False,\n",
              "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
              "                                   max_df=1.0, min_df=2, ngram_range=(1, 2),\n",
              "                                   preprocessor=None, stop_words='english',\n",
              "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                   tokenizer=None, top_n=20, verbose=False,\n",
              "                                   vocabulary=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40nBTa4UPrO2"
      },
      "source": [
        " ### Execute a query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:54:57.200016Z",
          "start_time": "2019-07-20T13:44:49.005187Z"
        },
        "id": "POH2gjywEcNb"
      },
      "source": [
        "query = 'The number of board meetings during the year 2019?'\n",
        "prediction = cdqa_pipeline.predict(query)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgdnYmW3P3d8"
      },
      "source": [
        "### Explore predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-07-20T13:54:57.336337Z",
          "start_time": "2019-07-20T13:54:57.318676Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThCffJekEdiC",
        "outputId": "0c2647f5-cf51-49ec-b3e8-79368c071f5b"
      },
      "source": [
        "print('query: {}'.format(query))\n",
        "print('answer: {}'.format(prediction[0]))\n",
        "print('title: {}'.format(prediction[1]))\n",
        "print('paragraph: {}'.format(prediction[2]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: The number of board meetings during the year 2019?\n",
            "answer: sixteen\n",
            "title: 23062020_SBI AR 2019-20 (Time 16_3b11)\n",
            "paragraph: MEETINGS OF THE CENTRAL BOARDThe Bank’s Central Board has to meet a minimum of six times in a year. During the year 2019-20, sixteen Central Board Meetings were held. The dates of the meetings and attendance of the directors are as under:\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}